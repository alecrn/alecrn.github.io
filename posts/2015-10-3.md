Better Software
===============

Software could be better.  This document outlines principles that should be
adhered, and ways that existing systems could improve.

Before-optimization principle
-----------------------------

Before optimizing something, recognize the essential model.  That which is most
self consistent, least error prone, most intuitive.

Start from this; you know that any optimizations you make will match this model.
This is how you avoid accidentally optimizing something in a way that introduces
inconsistency.

Minimal data principle
----------------------

Keep the smallest data possible, from which all that remains can be derived.

This is how systems should be designed.  Sure, caching is used for optimization.
But given the before-optimization principle, it should *act* like all the data
is derived from a kernel.

Truly compressed data is actually just an integer from a set of integers.  It's
impossible to create compressed data that's "wrong."  If it's possible for data
in a model to be inconsistent, then that's a facet which could be compressed
further.

Typically there is a difference between core and representation.

    Render :: Core -> Representation

Because the core is always self-consistent with respect to the model, all
updates are done on the core, rather than the representation.  Then the
representation is recomputed from the core.

I should find that article on optimizing a function that follows this pattern.

Languages
---------

Dependent typing needs to be more popular.

One of the cores of programming is the function.  Functions represent
dependency; you give me this, I'll give you back this.  Addition asks for two
numbers and returns a number, for instance.

Dependent typing makes it possible for functions to depend on proofs instead of
just what we'd normally call values.  For instance, a function to retrieve an
item from an array might look like

    getIndex(array of A, integer index, proof that index < len(array)) -> A

There could be a language for which the compilation target is a library.  That
is, instead of compiling program A to binary B, you run program A, which
produces binary B.

There'd be libraries for emitting machine code.  This also means you could
create your own DSL for CSS, HTMl, JavaScript, etc.

Using the same dependent programming ideas, this could be incredibly safe and
fast.  You could program at the assembly level in a completely safe way.

Also, like lisp, code should just be data.  This fits in with the idea the idea
that the program just generates code.  For instance if we have a function

    (def (hypotenuse a b) (sqrt (+ (** a 2) (** b 2))))

Or somesuch, the code is just code, ready to be interpreted by anything else.  A
javascript compiler could handle it one way, a native compiler a different way.
The compiler itself is programmed.

It'd be even safer than Rust.

It's time for silly errors to be gone from software.

This'd also make it VASTLY easier to make JITs.  For instance, since the
compiler code is just part of the language, it'd be easy to compile regular
expressions at runtime.  Or SQL queries.

Terminal
--------

Terminals are old and suck.

Replace it all with a graphical windowing system.

The advantage of a terminal is it's an extremely portable medium for programs.
You can easily SSH with a terminal.  It's dead easy to write utilities for a
terminal.

A program gets a box it can draw into.  It's just a grid of pixels.  Userspace
libraries handle text, etc.  Programs would be consistent across platforms and
very fast.

Built in would be a fast implementation of a React-like diff engine.  The
program would generate each frame from scratch and hand it to the windower.  If
the window is being viewed over a connection like SSH, it'd be serialized in a
very efficient manner.  Otherwise it could be directly updated in a quick way.

Programs would also have access to the raw stream so they can be more efficient
if necessary, like a video player.

The shell and tiling window system would pretty much be the same.  The shell
just allocates space/tiles to different subprograms.

This'd finally make it possible to view an image directly in the shell.  Or have
much better displays for data.

Editors
-------

There should be a tree based editor that edits the structure of things, not the
character grid.

For instance given

    <a href="/mylink">name</a>

You'd navigate it more like the following tree

    <a href="/mylink">name</a>
         <a href="/mylink">
            a
            href="/mylink"
                href
                "/mylink"
            name

If you typed a double quote into the quoted string, it'd automatically be
escaped to `&quot;`.

Operations that could be performed in the editor would include swapping
siblings, deleting nodes, adding nodes, editing nodes, etc.

Code would always have correct syntax and be correctly formatted, because it's
impossible to have it any other way.

Same as the minimal data principle.

Like react, you edit the core data, and the representation (text) is computed
from that.  Thus the code is always correct, and edits are optimal.

The editor could simultaneously edit the filesystem too, since that's a tree as
well.  However, that might be a bit harder to think about, as the file system
changes underneath the editor.

Either way similar tree-navigation keystrokes could be used.

Distribution/anarchy
--------------------

So much software depends on a client/server architecture.

This is generally for two reasons: to distribute the use of resources, and to
share data.

For instance, you don't need all of Netflix's movies on your computer, so you
use their storage space.  Plus when new movies and shows come out, you want to
watch them.

But there are some issues for this.  For one, the system completely depends on
the server to be up.  It's an obvious bottleneck point.  Plus, it doesn't make
sense that only one person can host the same thing; someone else should be able
to spin up their own server which does the same thing.

More software should be distributed.  Like git.  Almost exactly like git, in
many ways.

Operations occur on local data, which is then pushed/pulled as needed.

For instance, my calorie counter could push/pull data from a central server.
This is optional, but may be convenient.  It can be any server I wish.  And it's
just simple data.

Then my desktop can pull from the same data.

I'd operate on the data locally.  Hopefully with a nice GUI application.  But it
*could* be anything.  Since it's just data.  If it's just an sqlite3 file, I
could poke around it with my sqlite3 client if I wanted.

I imagine a system like git that's more general.  It handles more kinds of data,
and isn't in any way geared toward code.  Diffs would be binary diffs; maybe
even just gzip deltas (or something similar), or custom for the application.

It would handle pushing, pulling, dealing with multiple remotes and branches.

This raises the question of merging.

Monotonic data is always mergeable.  Which is important.

So most applications could use some kind of monotonic data, which takes care of
merging.

Another important kind of resource is computation.  It should be easy to
distribute computational resources, the same way you might with storage.

Take the source code for a server, and put it up on rented hardware.

Rented resources should be paid for.  Otherwise, who is, and why?

With the window manager, I could run programs on a different computer and stream
them to mine pretty easily.

Lots of existing websites would be fascinating if they were distributed.

BitTorrent already covers a lot of media sharing, like games, movies, music,
books, software, etc. etc.

It'd be great to see bittorrent for package managers.

The almost more interesting ones are more computational platforms.

For instance, tumblr, twitter.  These lend themselves to distribution.

Ebay and craigslist and other markets could be local.

The memex would be a great example that covers a lot of these.

Encryption and authentication are significant here.  It also implies a web of
trust.

Data market
-----------

It'd be cool to have a free market of data.  Like bittorrent, but you auction
off your data.

This would offset the costs of seeding, encourage people to seed, and encourage
people to find valuable data to seed.

Some people could even have a side business of scanning, e.g. textbooks and
seeding them.

As data distributes throughout the system, costs plummet, but probably staying
above a certain minimum amount to offset bandwidth/cpu cycle costs.

The hardest part might be verifying the correctness of data.  Since people would
probably try to cheat.  Money would have to be stored in escrow or somesuch.

Privacy/encryption
------------------

As many things should be encrypted/authenticated as possible.  This is important
in a distributed world, as well as a time of NSA spying.

Distributed, encrypted markets/networks could circumvent so much silliness.
